{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Customizing Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae68a918039e25cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.sam.modeling_sam import SamVisionAttention\n",
    "\n",
    "class SamVisionAttentionSplit(SamVisionAttention, nn.Module):\n",
    "    def __init__(self, config, window_size):\n",
    "        super().__init__(config, window_size)\n",
    "        # remove combined qkv\n",
    "        del self.qkv\n",
    "        # separate q, k, v projections\n",
    "        self.q = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)\n",
    "        self.k = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)\n",
    "        self.v = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)\n",
    "        \n",
    "        self._register_load_state_dict_pre_hook(self.split_q_k_v_load_hook)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4104ae9c98ad9f88"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    def split_q_k_v_load_hook(self, state_dict, prefix, *args):\n",
    "        keys_to_delete = []\n",
    "        for key in list(state_dict.keys()):\n",
    "            if \"qkv.\" in key:\n",
    "                # split q, k, v from the combined projection\n",
    "                q, k, v = state_dict[key].chunk(3, dim=0)\n",
    "                # replace with individual q, k, v projections\n",
    "                state_dict[key.replace(\"qkv.\", \"q.\")] = q\n",
    "                state_dict[key.replace(\"qkv.\", \"k.\")] = k\n",
    "                state_dict[key.replace(\"qkv.\", \"v.\")] = v\n",
    "                # mark the old qkv key for deletion\n",
    "                keys_to_delete.append(key)\n",
    "        \n",
    "        # remove old qkv keys\n",
    "        for key in keys_to_delete:\n",
    "            del state_dict[key]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d594fb4c6c397fd9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n",
    "        batch_size, height, width, _ = hidden_states.shape\n",
    "        qkv_shapes = (batch_size *  self.num_attention_heads,  height * width, -1)\n",
    "        query = self.q(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)\n",
    "        key = self.k(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)\n",
    "        value = self.v(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)\n",
    "\n",
    "        attn_weights = (query * self.scale) @ key.transpose(-2, -1)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn_weights = self.add_decomposed_rel_pos(\n",
    "                attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n",
    "            )\n",
    "\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = (attn_probs @ value).reshape(batch_size, self.num_attention_heads, height, width, -1)\n",
    "        attn_output = attn_output.permute(0, 2, 3, 1, 4).reshape(batch_size, height, width, -1)\n",
    "        attn_output = self.proj(attn_output)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs = (attn_output, attn_weights)\n",
    "        else:\n",
    "            outputs = (attn_output, None)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c00720cc901858e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
