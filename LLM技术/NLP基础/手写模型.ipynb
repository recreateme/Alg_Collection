{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 手写模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ad0a62a138859d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5bcb4f1698b1cb4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T08:07:06.331197600Z",
     "start_time": "2025-03-26T08:07:03.347475700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "d_model = 512  # 词向量维度\n",
    "context_size = 16  # 上下文窗口大小\n",
    "num_heads = 8  # 多头注意力机制的头数\n",
    "head_dim = d_model // num_heads  # 每个头的维度\n",
    "dropout = 0.1  # 随机失活率\n",
    "num_blocks = 6  # 编码器块数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T08:07:57.864146300Z",
     "start_time": "2025-03-26T08:07:57.847035100Z"
    }
   },
   "id": "8f597063ac2b7c14",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ffn层\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.ffn == nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        self.ffn(x)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T08:10:47.781877300Z",
     "start_time": "2025-03-26T08:10:47.765316200Z"
    }
   },
   "id": "e0367b048432ba32",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 注意力层\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wq = nn.Linear(d_model, head_dim)  \n",
    "        self.Wk = nn.Linear(d_model, head_dim)\n",
    "        self.Wv = nn.Linear(d_model, head_dim)\n",
    "        # 掩码\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(context_size, context_size)))\n",
    "    def forward(self, x):\n",
    "        # x是[batch_size, seq_len, d_model]，其中seq_len是序列长度，范围是[1, seq_len]\n",
    "        # mask是[batch_size, 1, seq_len, seq_len]，其中mask[i,j,k,l]表示第i个样本的第k个位置是否可以看做第j个样本的第l个位置\n",
    "        # 注意力权重\n",
    "        B, S, D = x.shape\n",
    "        q = self.Wq(x)  # [batch_size, seq_len, head_dim]\n",
    "        k = self.Wk(x)  \n",
    "        v = self.Wv(x)  # [batch_size, seq_len, head_dim]\n",
    "        \n",
    "        output = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(k.size(-1))\n",
    "        # 掩码\n",
    "        output = output.masked_fill(self.mask[:S, :S] == 0, float('-inf'))  # 掩掉对角线，即不允许看做自己\n",
    "        output = F.softmax(output, dim=-1)  # [batch_size, seq_len, seq_len]\n",
    "        output = nn.Dropout(self.dropout)(output)\n",
    "        \n",
    "        # 输出\n",
    "        output = torch.matmul(output, v)  \n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T08:36:43.332450200Z",
     "start_time": "2025-03-26T08:36:43.308700600Z"
    }
   },
   "id": "7d2110577fd597d7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 多头注意力层\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList([Attention() for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        output = self.dropout(self.linear(output))\n",
    "        \n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T08:47:11.234743500Z",
     "start_time": "2025-03-26T08:47:11.214071300Z"
    }
   },
   "id": "9e298ec942af8b95",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Transformer块\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention()\n",
    "        self.ffn = Feedforward()\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.layernorm1(x))\n",
    "        x = x + self.ffn(self.layernorm2(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T09:32:37.047308700Z",
     "start_time": "2025-03-26T09:32:37.031810500Z"
    }
   },
   "id": "9afc8a7096897720",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SimplifiedTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        # 1. 移除残差连接和归一化层，保留核心注意力+MLP\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads=8)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*d_model, d_model)\n",
    "        )\n",
    "        # 2. 使用Value-SkipInit稳定训练（论文[7](@ref)方案）\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.1))\n",
    "        self.beta = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        attn_out = self.alpha * x + self.beta * attn_out  # 替代残差连接\n",
    "        mlp_out = self.mlp(attn_out)\n",
    "        return mlp_out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T09:49:38.056207200Z",
     "start_time": "2025-03-26T09:49:38.048517700Z"
    }
   },
   "id": "37277230f8f85303",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "class BlockTransformer(nn.Module):\n",
    "    def __init__(self, d_model, block_size=4):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        # 块内局部注意力\n",
    "        self.local_attn = TransformerEncoder(\n",
    "            TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=4*d_model),\n",
    "            num_layers=2\n",
    "        )\n",
    "        # 块间全局注意力\n",
    "        self.global_attn = TransformerEncoder(\n",
    "            TransformerEncoderLayer(d_model, nhead=8),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        x = x.view(B, T//self.block_size, self.block_size, D)\n",
    "        x = self.local_attn(x)          # 处理块内依赖\n",
    "        x = x.mean(dim=2)               # 块嵌入向量\n",
    "        x = self.global_attn(x)         # 处理块间依赖\n",
    "        return x.repeat_interleave(self.block_size, dim=1)[:,:T]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T09:50:31.527455800Z",
     "start_time": "2025-03-26T09:50:31.514380Z"
    }
   },
   "id": "61fb641ec1acb3f9",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 位置编码优化\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度用sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度用cos\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # 形状(1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2e9dbc89da53e67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class OptimizedModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_blocks=6):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.vocab_linear = nn.Linear(d_model, vocab_size)\n",
    "        # 使用简化Transformer块+块级注意力混合架构\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[SimplifiedTransformerBlock(d_model) if i < num_blocks//2 \n",
    "              else BlockTransformer(d_model) for i in range(num_blocks)],\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "        # 位置编码优化（直接调用PyTorch内置）\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        x = self.embedding(x_batch) * math.sqrt(d_model)\n",
    "        x = self.pos_encoder(x)  # 替代手动实现\n",
    "        return self.vocab_linear(self.transformer(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d31b28dc2d6213c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
