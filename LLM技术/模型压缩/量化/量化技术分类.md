## 量化对象

模型量化的对象主要包括以下几个方面：

- **权重**（weight）：weight的量化是最常规也是最常见的。量化weight可达到减少模型大小内存和占用空间。
- **激活**（activation）：实际中activation往往是占内存使用的大头，因此量化activation不仅可以大大减少内存占用。更重要的是，结合weight的量化可以充分利用整数计算获得性能提升。
- **KV cache**：量化 KV 缓存对于提高长序列生成的吞吐量至关重要。
- **梯度**（Gradients）：相对上面两者略微小众一些，因为主要用于训练。在训练深度学习模型时，梯度通常是浮点数，它主要作用是在分布式计算中减少通信开销，同时，也可以减少backward时的开销。

### 一、基础量化技术分类

#### 1. **训练后量化（Post-Training Quantization, PTQ）**
• **原理**：直接对预训练模型进行低精度转换，无需重新训练。通过校准数据确定量化参数范围。
• **典型方法**：
  • **静态量化**：固定量化参数（如权重和激活范围），适用于输入分布稳定的场景（如GPTQ）
  • **动态量化**：运行时根据输入动态计算量化参数，适合输入变化大的任务（如对话系统）
• **优势**：部署快速，硬件兼容性好，显存占用减少75%
• **案例**：LLaMA-13B经GPTQ量化后，模型体积从26GB压缩至7GB，推理速度提升3倍

#### 2. **量化感知训练（Quantization-Aware Training, QAT）**
• **原理**：在训练阶段模拟量化误差，通过反向传播优化量化鲁棒性
• **关键技术**：
  • **对称MinMax量化**：保护权重和激活的异常值分布（如LLM-QAT）
  • **低精度梯度近似**：使用STE（Straight-Through Estimator）保持训练稳定性
• **优势**：精度损失<1%，适合医疗诊断等高精度需求场景
• **局限**：需要重新训练，计算成本较高

#### 3. **混合精度量化**
• **原理**：关键层保留FP16/FP8精度，其他层量化至INT8/INT4
• **实现方式**：
  • **层级敏感度分析**：通过Hessian矩阵识别敏感层（如输出层保持FP16）
  • **动态切换机制**：根据计算需求自动调整精度（如NVIDIA TensorRT）
• **优势**：平衡速度与精度，在T4 GPU上推理速度提升40%

### 二、前沿量化技术突破

#### 1. **非均匀量化（Non-uniform Quantization）**
• **原理**：打破均匀量化间隔，基于参数分布动态调整量化区间
  • **K-means聚类**：根据权重聚类结果分配量化中心（如QAT中的可学习量化器）
  • **信息熵优化**：高信息量区域分配更多比特（如OPT-175B的4bit量化）
• **效果**：4bit非均匀量化达到8bit均匀量化的精度水平

#### 2. **稀疏-量化联合优化**
• **技术路线**：
  1. **结构化剪枝**：移除80-90%冗余参数
  2. **低精度量化**：对剩余参数进行4bit压缩
• **案例**：LLaMA-13B经剪枝+量化后，体积从48GB→4.8GB，速度提升5倍

#### 3. **激活感知量化（AWQ）**
• **创新点**：根据激活分布保护重要通道，减少量化误差
  • **数学表达**：权重缩放因子 $s = \max(|W|)/2^{b-1}$，激活缩放因子 $s_a = \max(|A|)/2^{b-1}$
• **效果**：LLaMA-7B量化后精度损失<1%

---

根据量化参数s和z的共享范围（即量化粒度），量化方法可以分为**逐层量化（per-tensor）**、**逐通道（per-token & per-channel 或者 vector-wise quantization ）量化和逐组量化（per-group、Group-wise）**。

### 三、硬件适配与系统优化

#### 1. **量化编译器支持**
• **TensorRT-LLM**：支持GPTQ/AWQ量化模型部署，A100 GPU推理延迟降低60%
• **vLLM**：结合PagedAttention实现量化模型的显存分页管理

#### 2. **量化-微调协同（QLoRA）**
• **技术融合**：
  • **4bit NF4量化**：保留预训练权重正态分布特性
  • **低秩适配器**：添加可训练LoRA参数补偿量化损失
• **优势**：单卡48GB GPU可微调65B模型，性能接近全精度微调

---

### 四、技术选型建议
| **场景需求** | **推荐技术**      | **参数量/显存** | **适用模型规模** |
| ------------ | ----------------- | --------------- | ---------------- |
| 快速部署     | GPTQ + 静态量化   | INT4（7GB）     | >10B参数模型     |
| 高精度要求   | AWQ/QAT           | INT8（13GB）    | 1-70B参数模型    |
| 边缘设备推理 | 稀疏+4bit联合量化 | 4.8GB           | 7-13B参数模型    |
| 持续微调     | QLoRA + NF4量化   | 0.1%可训练参数  | 7-65B参数模型    |

---

### 五、未来趋势
1. **量化-蒸馏融合**：结合知识蒸馏提升低精度模型容量（如Distil-Quant）
2. **极端量化探索**：3bit/2bit量化在中小模型的应用（如Ternary-Weight Networks）
3. **多模态扩展**：图文联合量化技术（如Stable Diffusion XL的4bit适配）

当前主流框架如Hugging Face Transformers、LMDeploy等均已集成上述量化工具，开发者可通过`bitsandbytes`等库快速实现部署。