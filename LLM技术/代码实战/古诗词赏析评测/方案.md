明白了。我将为你制定一个基于Transformers + PEFT（LoRA）的古诗词理解与情感推理监督微调方案，使用不超过20B参数量的开源模型，并结合混合精度训练以适应32G显存。方案将涵盖模型选择、数据预处理、LoRA配置、训练流程、评估指标实现等内容。 我会尽快整理好给你。

# 古诗词理解与情感推理微调方案

## 模型推荐

考虑到任务为中文古诗理解与情感分类，且硬件仅有单卡32GB显存，我们优先选择开源且中文表现优秀、参数量 ≤ 20B 的模型。可选模型包括：**ChatGLM-6B/ChatGLM3-6B**（6B参数，针对中文对话与问答优化，开源可商用） ([GitHub - HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM#:~:text=* ChatGLM： * 地址：https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM,简介：中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持))；**百川 Baichuan-7B**（7B参数，中英双语预训练，中文基准性能优异） ([GitHub - HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM#:~:text=* Baichuan,7B 之后开发的包含 130))；**Baichuan-13B**（13B参数，中文综合能力强） ([GitHub - HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM#:~:text=* 简介：由百川智能开发的一个开源可商用的大规模预训练语言模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持 中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C,Chat) 两个版本。 * Baichuan2))；以及**书生·浦语 InternLM-7B**（7B参数，由上海AI实验室开源，轻量而性能不俗） ([GitHub - HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM#:~:text=简介：商汤科技、上海AI实验室联合香港中文大学、复旦大学和上海交通大学发布千亿级参数大语言模型“书生·浦语”（InternLM2）。InternLM2 在数理、代码、对话、创作等各方面能力都获得了长足进步，综合性能达到开源模型的领先水平。InternLM2 包含两种模型规格：7B 和 20B。7B,V2))。在32GB显存下，使用 LoRA 等技术后，7B~13B 模型均可训练，其中 7B 模型最为稳妥，13B 可尝试（必要时可配合 8-bit 量化）。以上模型均支持 Transformers + PEFT 微调，且适合中文任务 ([GitHub - HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM#:~:text=* ChatGLM： * 地址：https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM,简介：中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持)) ([GitHub - HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM#:~:text=* Baichuan,7B 之后开发的包含 130))。

## 数据预处理与样本构造（Prompt 形式）

- **加载与提取字段**：读取训练集 JSON（包含 `title`、`author`、`content`、`keywords`、`trans`、`emotion` 等字段）。将`content`按照诗行分割为句子列表（如按换行或句号分隔），将`keywords`字典的键或值提取为关键词列表。

- **构造提示词（Prompt）**：设计统一的提示格式，让模型同时完成关键词/句子解释与情感分类。例如：

  ```
  诗名：《title》，作者：author，内容：content  
  关键词：keyword1, keyword2, ...  
  句子：sentence1； sentence2； ...  
  请完成以下任务：  
  1. 对每个关键词生成现代汉语解释；  
  2. 对每个句子生成白话文解释；  
  3. 对整首诗进行情感分类（选项0/1/2/3）。  
  输出格式：  
  ans_qa_words: [解释1, 解释2, ...]  
  ans_qa_sents: [解释1, 解释2, ...]  
  choose_id: X
  ```

  其中 **ans_qa_words** 对应关键词列表顺序的解释，**ans_qa_sents** 对应句子列表顺序的解释，**choose_id** 为情感分类结果（0~3）。

- **标签构造**：利用训练数据的 `trans`（全文译文）和 `keywords` 中的释义：将 `trans` 按句分割得到每行对应的白话句子解释，作为 `ans_qa_sents`；如果 `keywords` 字典值为释义，可直接作为对应的 `ans_qa_words` 输出，否则可以参考字典或工具获取关键词的现代意义作为标签。情感标签 `emotion` 对应 `choose_id`，根据数据定义映射为 0~3 中的某个值。

- **示例**：

  ```jsonc
  // 输入（Prompt）示例：  
  诗名：《title》，作者：author，内容：content（诗词原文）  
  关键词：关键词1, 关键词2, ...  
  句子：句子1； 句子2； ...  
  任务：请为每个关键词和每个句子生成白话解释，并对全诗进行情感分类（0~3）。  
  输出格式：ans_qa_words, ans_qa_sents, choose_id。  
  
  // 输出（Target）示例：  
  ans_qa_words: ["关键词1的解释", "关键词2的解释", ...]  
  ans_qa_sents: ["句子1的解释", "句子2的解释", ...]  
  choose_id: 2
  ```

这样构造问答对后，可直接用于 Transformers 模型的监督微调。

## Transformers + PEFT 训练流程

- **环境与库**：使用 Hugging Face Transformers 加载预训练模型和 tokenizer，并安装 PEFT 库。

- **加载模型**：例如使用 `AutoModelForCausalLM.from_pretrained(model_name)` 载入基础模型。

- **添加 LoRA Adapter**：创建 LoRA 配置并应用于模型，例如：

  ```python
  from transformers import AutoModelForCausalLM
  from peft import LoraConfig, get_peft_model, TaskType
  model = AutoModelForCausalLM.from_pretrained(model_name)  # 基础模型
  lora_config = LoraConfig(
      task_type=TaskType.CAUSAL_LM,
      r=8,                 # LoRA 低秩矩阵维度
      lora_alpha=32,       # 缩放因子
      lora_dropout=0.1,
      target_modules=["q_proj", "v_proj"]  # 视模型结构决定
  )
  model = get_peft_model(model, lora_config)
  ```

  PEFT (低秩适应) 方法只对少量新增参数 (adapter) 进行训练，**冻结了原模型权重**，极大节省显存 ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C a library of parameter,share%2C store%2C and load them))。常见取值如 `r=8, alpha=32` 可作为起点 ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=lora_config %3D LoraConfig( task_type%3DTaskType.CAUSAL_LM%2C ,dropout of LoRA layers))。

- **训练设置**：使用 `TrainingArguments` 或 `Seq2SeqTrainingArguments`（视模型结构），启用混合精度 `fp16=True`、梯度检查点 `gradient_checkpointing=True`（节省显存）等；设置适当的学习率和微批大小，可用 `per_device_train_batch_size=1` 配合多次累积更新来模拟更大批次。示例：

  ```python
  from transformers import Trainer, TrainingArguments
  training_args = TrainingArguments(
      output_dir="output",
      num_train_epochs=5,
      learning_rate=1e-4,
      per_device_train_batch_size=1,
      gradient_accumulation_steps=8,
      fp16=True,
      gradient_checkpointing=True,
      logging_steps=10,
      save_steps=100
  )
  trainer = Trainer(model=model, args=training_args, train_dataset=train_ds, eval_dataset=val_ds)
  trainer.train()
  ```

- **验证与保存**：训练过程中定期在验证集上计算损失或指标；训练完成后，使用 `model.save_pretrained()` 或 `model.save_adapter()` 保存模型和 LoRA 权重 ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=Save your adapter with save_pretrained,to reuse it))。后续推理时加载基础模型并应用同样的 LoRA adapter。

## 混合精度和内存优化技巧

- **梯度检查点 (Gradient Checkpointing)**：开启 `gradient_checkpointing=True` 可以在反向传播时只保存少量激活值，余下部分重新计算，从而显著降低显存占用 ([Performance and Scalability: How To Fit a Bigger Model and Train It Faster](https://huggingface.co/docs/transformers/v4.19.4/en/performance#:~:text=To enable gradient checkpointing in,is handled under the hood)) ([Performance and Scalability: How To Fit a Bigger Model and Train It Faster](https://huggingface.co/docs/transformers/v4.19.4/en/performance#:~:text=One way to use significantly,When enabled%2C a lot))。这是 Hugging Face 推荐的在单卡上训练大模型的技巧。开启后训练速度会稍慢（大约20%），但可支持更大模型。
- **混合精度训练 (FP16)**：启用 `fp16=True` 让模型在半精度下运算和存储参数，可将每个参数占用从32字节降为16字节，从而近乎减半显存需求 ([Performance and Scalability: How To Fit a Bigger Model and Train It Faster](https://huggingface.co/docs/transformers/v4.19.4/en/performance#:~:text=One way to use significantly,When enabled%2C a lot))。Hugging Face Trainer 或 Accelerate 框架可自动处理标量汇总和梯度缩放。
- **低精度量化**：对于 8B+ 的模型，可考虑使用 bitsandbytes 的 8-bit 或 4-bit 量化加载，例如 `model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True)`。这进一步降低显存占用，允许在 32GB 卡上训练较大模型，但可能稍微影响精度。
- **小批次与累积**：由于显存限制，通常设置微批量（如1）并通过 `gradient_accumulation_steps` 叠加梯度来模拟较大批量。这与梯度检查点、FP16 结合使用效果更佳。
- **LoRA 参数高效**：LoRA 本身只引入极少数可训练参数，因此相比全模型微调占用显存更少 ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C a library of parameter,share%2C store%2C and load them))。总之，通过上述策略，可在 32GB 显卡上完成 7B~13B 模型的微调。

## 评估指标实现方案

- **BLEU 分数**：用于评估生成的词语/句子解释的准确性。将模型生成的 `ans_qa_words` 列表和 `ans_qa_sents` 列表分别与参考解释进行对比，计算 BLEU（n-gram 重叠）指标。可使用 `evaluate.load("bleu")` 计算 BLEU ([BERTScore in AI: Transforming Semantic Text Evaluation and Quality - Galileo AI](https://www.galileo.ai/blog/bert-score-explained-guide#:~:text=As organizations increasingly adopt Generative,the ROUGE metric in AI))。

- **中文 BERTScore**：BERTScore 利用预训练语言模型计算候选文本与参考文本的语义相似度，能捕捉同义或近义表达的匹配。使用 `evaluate.load("bertscore", lang="zh")`（通常基于中文 BERT 或多语言模型）进行评估 ([BERTScore in AI: Transforming Semantic Text Evaluation and Quality - Galileo AI](https://www.galileo.ai/blog/bert-score-explained-guide#:~:text=At the core of BERTScore,precise reflection of semantic equivalence))。BERTScore 比 BLEU 更关注语义一致性，适合中文生成任务。

- **准确率 (Accuracy)**：针对情感多项选择，直接计算预测的 `choose_id` 与真实 `emotion` 标签的匹配率。可用 `evaluate.load("accuracy")` 或自行统计 `correct/total`。

- **实现方法**：将验证集或测试集的模型输出与参考答案分别收集，例如：

  ```python
  from evaluate import load
  bleu = load("bleu")
  bertscore = load("bertscore", lang="zh")
  acc = load("accuracy")
  # 假设 preds_words, refs_words, preds_sents, refs_sents 为列表
  bleu_words = bleu.compute(predictions=preds_words, references=[[r] for r in refs_words])
  bert = bertscore.compute(predictions=preds_words+preds_sents, references=[refs_words+refs_sents]*2)
  accuracy = acc.compute(predictions=pred_choose, references=ref_choose)
  ```

  通过这些指标可以全面评估微调模型的翻译质量和分类性能。

## 训练/验证/推理代码逻辑结构建议

- **数据处理模块**：使用 `datasets.load_dataset("json", data_files=...)` 读取 JSON 文件，定义 `map` 函数将原始字段拼接为输入提示，并生成对应的输出标签（as described above）。使用 `tokenizer` 对提示和标签进行编码，注意截断长度等。

- **模型与 LoRA 设置**：初始化模型和 LoRA 配置（见上文），通过 `model = get_peft_model(model, lora_config)` 应用 LoRA。确认模型所有层冻结，只有 adapter 更新。

- **Trainer 训练流程**：配置 `TrainingArguments`（启用 FP16、梯度检查点、学习率等），并用 `Trainer` 绑定模型、数据集、评估函数等。定义 `data_collator` 如果需要（如自动padding）。训练中可在 `eval_strategy="epoch"` 或按步进行验证，并保存最优模型。

- **推理流程**：训练后加载基础模型和 LoRA adapter（使用 `PeftModel.from_pretrained`），对于测试集每条输入（同样的 prompt 格式），使用 `model.generate()` 生成完整输出字符串。

- **解析输出**：将模型生成的文本解析为结构化字段。例如，可以寻找关键字 `"ans_qa_words:"`、`"ans_qa_sents:"`、`"choose_id:"`，并提取后面的列表或数字，转换成 Python 对象（可使用正则或 JSON 解析方法）。

- **示例代码结构**：

  ```python
  # 数据加载和预处理
  ds = load_dataset("json", data_files={"train":"train.json", "validation":"val.json"})
  ds = ds.map(preprocess_function)  # 将原数据构造成 prompt-answer 对
  # 模型和 LoRA
  model = AutoModelForCausalLM.from_pretrained(model_name)
  model = get_peft_model(model, lora_config)
  # 训练
  trainer = Trainer(model, training_args, train_dataset=ds["train"], eval_dataset=ds["validation"])
  trainer.train()
  # 推理
  model.eval()
  output = model.generate(**tokenizer(test_prompt))
  parsed = parse_output(output_text)
  ```

  整体保持模块化，易于调试和复现。

## 推理输出格式与后处理

- **统一格式**：推理时需保证模型输出严格遵循 `ans_qa_words`、`ans_qa_sents`、`choose_id` 三部分。如可能，指示模型输出 JSON 格式字典或明确的键值标签，以便后续解析。

- **解析方法**：可先将生成字符串按 `ans_qa_words:`、`ans_qa_sents:`、`choose_id:` 分割，提取列表内容并去除多余符号。例如，使用正则 `r"ans_qa_words:\s*\[(.*?)\]"` 捕获关键词解释列表，`r"choose_id:\s*(\d+)"` 捕获数字类别。

- **输出验证**：对每条输出，检查 `choose_id` 是否为 0~3 的整数，且 `ans_qa_words` 与 `ans_qa_sents` 长度与关键词/句子数一致。若解析失败，可记录错误实例进行人工检查。

- **最终格式**：将结果整理为所需 JSON 结构，例如：

  ```json
  {
    "ans_qa_words": ["...解释1...", "...解释2...", ...],
    "ans_qa_sents": ["...解释1...", "...解释2...", ...],
    "choose_id": 2
  }
  ```

  这样既满足题目要求，也方便后续自动评分或提交。

## 轻量化部署建议

- **合并保存模型**：训练完成后，可使用 `model.save_pretrained()` 保存基础模型和 LoRA adapter ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=Save your adapter with save_pretrained,to reuse it))。如果只需要部署微调结果，可以使用 PEFT 提供的 `merge_and_unload()` （或等效操作）将 LoRA 参数融合入模型权重，得到一个纯权重模型，便于推理部署。
- **量化推理**：部署时可继续使用 8-bit/4-bit 量化或 ONNX 量化技术，进一步降低显存和加速推理。Hugging Face 的 `optimum` 或 `bitsandbytes` 可帮助导出量化模型。
- **API/服务**：可将模型封装为 API 服务，如使用 Hugging Face Inference Endpoint、TorchServe 或 FastAPI。对于实时性要求较高的场景，考虑线程安全或并发控制，或使用 GPU 加速。
- **模型蒸馏**：若硬件极其受限，可考虑蒸馏到更小的学生模型，例如蒸馏到较小的 3B 模型或 ChatGLM-6B，以牺牲部分准确度换取性能。
- **总结**：合理合并 LoRA、量化和加速框架可使该中文诗歌理解情感模型在资源有限的环境中高效部署。在保证准确度的前提下，以上方法可显著减小内存使用和推理延迟 ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=Save your adapter with save_pretrained,to reuse it))。

**参考资料：** Baichuan-7B/13B 和 ChatGLM 等中文大模型在各种基准上表现出色 ([GitHub - HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM#:~:text=* ChatGLM： * 地址：https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM,简介：中文领域效果最好的开源底座模型之一，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持)) ([GitHub - HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM#:~:text=* Baichuan,7B 之后开发的包含 130))；PEFT LoRA 方法仅微调少量参数极大节省显存 ([PEFT](https://huggingface.co/docs/transformers/en/peft#:~:text=PEFT%2C a library of parameter,share%2C store%2C and load them))；梯度检查点和混合精度是单卡训练大模型的标准做法 ([Performance and Scalability: How To Fit a Bigger Model and Train It Faster](https://huggingface.co/docs/transformers/v4.19.4/en/performance#:~:text=To enable gradient checkpointing in,is handled under the hood)) ([Performance and Scalability: How To Fit a Bigger Model and Train It Faster](https://huggingface.co/docs/transformers/v4.19.4/en/performance#:~:text=One way to use significantly,When enabled%2C a lot))；BLEU 和 BERTScore 可分别衡量翻译输出的精确度和语义相似度 ([BERTScore in AI: Transforming Semantic Text Evaluation and Quality - Galileo AI](https://www.galileo.ai/blog/bert-score-explained-guide#:~:text=At the core of BERTScore,precise reflection of semantic equivalence)) ([BERTScore in AI: Transforming Semantic Text Evaluation and Quality - Galileo AI](https://www.galileo.ai/blog/bert-score-explained-guide#:~:text=As organizations increasingly adopt Generative,the ROUGE metric in AI))。以上方法结合可构建完整的古诗词理解与情感推理微调方案。